\select@language {ngerman}
\contentsline {section}{\numberline {1}Lecture April 18th}{3}{section.1}
\contentsline {paragraph}{\nonumberline What is Statistical Learning? (2.1)}{3}{section*.2}
\contentsline {paragraph}{\nonumberline Why estimate $f$? (2.1.1)}{3}{section*.3}
\contentsline {section}{\numberline {2}Lecture April 19th}{5}{section.2}
\contentsline {paragraph}{\nonumberline How do we estimate $f$? (2.1.2)}{6}{section*.4}
\contentsline {section}{\numberline {3}Lecture April 25th}{8}{section.3}
\contentsline {paragraph}{\nonumberline Supervised vs. unsupervised learning (2.1.4)}{9}{section*.5}
\contentsline {paragraph}{\nonumberline Regression vs. classification (2.1.5)}{9}{section*.6}
\contentsline {paragraph}{\nonumberline Assessing model accuracy (2.2)}{10}{section*.7}
\contentsline {section}{\numberline {4}Lecture May 2nd}{12}{section.4}
\contentsline {section}{\numberline {5}Lecture May 3rd}{15}{section.5}
\contentsline {paragraph}{\nonumberline The bias variance trade-off (2.2.2)}{15}{section*.8}
\contentsline {paragraph}{\nonumberline The classification setting (2.2.3)}{16}{section*.9}
\contentsline {section}{\numberline {6}Lecture May 9th}{18}{section.6}
\contentsline {section}{\numberline {7}Lecture, May 17th}{21}{section.7}
\contentsline {paragraph}{\nonumberline Estimating the coeffictions (3.1.1)}{22}{section*.10}
\contentsline {paragraph}{\nonumberline Multiple Linear Regression (3.2)}{24}{section*.11}
\contentsline {section}{\numberline {8}Lecture May 23rd}{26}{section.8}
\contentsline {paragraph}{\nonumberline Other considerations in the linear regression model (3.3)}{26}{section*.12}
\contentsline {paragraph}{\nonumberline Qualitative predictors (3.3.1)}{27}{section*.13}
\contentsline {section}{\numberline {9}Lecture May 24th}{29}{section.9}
\contentsline {paragraph}{\nonumberline Classification (Chapter 4)}{32}{section*.14}
\contentsline {paragraph}{\nonumberline Why not linear regression (4.2)}{33}{section*.15}
\contentsline {section}{\numberline {10}Lecture May 30th}{35}{section.10}
\contentsline {paragraph}{\nonumberline Logistic Regression Mode (4.3.1)}{35}{section*.16}
\contentsline {paragraph}{\nonumberline Estimating the regression coefficients (4.3.2)}{35}{section*.17}
\contentsline {paragraph}{\nonumberline Making predictions (4.3.3)}{36}{section*.18}
\contentsline {paragraph}{\nonumberline Multiple logistic regression (4.3.4)}{36}{section*.19}
\contentsline {paragraph}{\nonumberline Logistic regression with > 2 response classes (4.3.5)}{37}{section*.20}
\contentsline {section}{\numberline {11}Lecture June 7th}{38}{section.11}
\contentsline {paragraph}{\nonumberline Linear discriminant analysis for $p = 1$ (4.4.2)}{39}{section*.21}
\contentsline {section}{\numberline {12}Lecture June 13th}{42}{section.12}
\contentsline {section}{\numberline {13}Lecture June 14th}{46}{section.13}
\contentsline {paragraph}{\nonumberline Resampling Methods (Chapter 5)}{47}{section*.22}
\contentsline {paragraph}{\nonumberline Cross validation (5.1)}{47}{section*.23}
\contentsline {paragraph}{\nonumberline Validation set approach (5.1.1)}{47}{section*.24}
\contentsline {paragraph}{\nonumberline Leave-one-out Cross-validation (5.1.2)}{48}{section*.25}
\contentsline {section}{\numberline {14}Lecture June 20th}{49}{section.14}
\contentsline {paragraph}{\nonumberline $K$-fold Cross-Validation (5.1.3)}{49}{section*.26}
\contentsline {paragraph}{\nonumberline Bias-Variance Trade-Off for $K$-fold Cross-Validation (5.1.4)}{49}{section*.27}
\contentsline {section}{\numberline {15}Lecture June 21st}{51}{section.15}
\contentsline {paragraph}{\nonumberline Chapter 6: Linear model selection and regularization}{51}{section*.28}
\contentsline {paragraph}{\nonumberline Subset Selection (6.1)}{51}{section*.29}
\contentsline {section}{\numberline {16}Lecture June 28th}{53}{section.16}
\contentsline {paragraph}{\nonumberline Stepwise selection (6.1.2)}{53}{section*.30}
\contentsline {paragraph}{\nonumberline Ridge Regression (6.2.1)}{53}{section*.31}
\contentsline {section}{\numberline {17}Lecture July 3rd}{57}{section.17}
\contentsline {paragraph}{\nonumberline High-Dimensional Data (6.4.1)}{58}{section*.32}
\contentsline {paragraph}{\nonumberline What goes wrong in high dimensions? (6.4.2)}{58}{section*.33}
\contentsline {paragraph}{\nonumberline Regression in high dimensions (6.4.3)}{59}{section*.34}
\contentsline {paragraph}{\nonumberline Interpreting Results in High Dimensions (6.4.4)}{59}{section*.35}
\contentsline {section}{\numberline {18}Lecture July 4th}{60}{section.18}
\contentsline {paragraph}{\nonumberline Chapter 8: Tree Based-Methods}{60}{section*.36}
\contentsline {section}{\numberline {19}Lecture July 5th}{64}{section.19}
\contentsline {paragraph}{\nonumberline Bagging, Random Forests, Boosting (8.2)}{65}{section*.37}
\contentsline {section}{\numberline {20}Lecture July 11th}{69}{section.20}
